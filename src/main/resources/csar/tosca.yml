tosca_definitions_version: alien_dsl_2_0_0

metadata:
  template_name: org.alien4cloud.k8s.spark.jobs
  template_version: 3.0.0
  template_author: a4c

imports:
  - tosca-normative-types:1.0.0-ALIEN20
  - alien-base-types:3.0.0
  - docker-types:3.0.0
  - org.alien4cloud.kubernetes.api:3.1.0
  - yorc-types:1.1.0

description: >
  Spark Jobs on k8s types.

data_types:

  org.alien4cloud.k8s.spark.jobs.Volume:
    derived_from: tosca.datatypes.Root
    properties:
      name:
        type: string
        description: |
          Name of the volume.
      type:
         type: string
         description: |
           Type of the volume
         constraints:
           valid_values: [ 'hostPath', 'emptyDir', 'persistentVolumeClaim' ]
      mountPath:
        type: string
        description: |
          Where to mount the volume.
      options:
        type: map
        required: false
        entry_schema:
          type: string
        description: |
          Mount options.

  org.alien4cloud.k8s.spark.jobs.PersistentVolumeClaimSpec:
    derived_from: tosca.datatypes.Root
    properties:
      claimName:
        type: string
        required: false
        description: |
          ClaimName is the name of a PersistentVolumeClaim in the same namespace as the pod using this volume. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
      readOnly:
        type: boolean
        required: false
        description: |
          Will force the ReadOnly setting in VolumeMounts. Default false.
        default: false


node_types:

  org.alien4cloud.k8s.spark.jobs.PersistentVolumeClaimSource:
    derived_from: tosca.nodes.Root
    description: |
      PersistentVolumeClaimVolumeSource represents a reference to a PersistentVolumeClaim in the same namespace.
      More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
    properties:
      name:
        type: string
      size:
        type: scalar-unit.size
        description: |
          The requested storage size (default unit is MB).
      spec:
        type: org.alien4cloud.k8s.spark.jobs.PersistentVolumeClaimSpec
        required: false
      accessModes:
        description: |
          AccessModes contains the desired access modes the volume should have.
          More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1
        type: string
        default: ReadWriteOnce
        constraints:
          - valid_values: [ReadWriteOnce, ReadOnlyMany, ReadWriteMany]
      selector:
        description: |
          A label query over volumes to consider for binding
        type: org.alien4cloud.kubernetes.api.datatypes.LabelSelector
        required: false
    requirements:
      - attachment:
          capability: org.alien4cloud.capabilities.DockerVolumeAttachment
          relationship: org.alien4cloud.relationships.MountDockerVolume
          occurrences: [1, unbounded]

  org.alien4cloud.k8s.spark.jobs.PersistentVolumeClaimStorageClassSource:
    derived_from: org.alien4cloud.k8s.spark.jobs.PersistentVolumeClaimSource
    description: |
      An extension of PersistentVolumeClaimSource that allows to specify a storageClass.
      If no volume that match this claim is found in the PersistentVolume pool, a volume will be allocated in the IAAS.
      When using this kind of volume, your Kube admin should ensure to have a storageClass defined in the cluster.
    properties:
      storageClassName:
        type: string
        default: default

  org.alien4cloud.k8s.spark.jobs.AbstractSparkJob:
    abstract: true
    derived_from: org.alien4cloud.nodes.Job
    description: |
      Abstract Spark Jobs on K8S.
    metadata:
      icon: /images/spark.png
    properties:
      var_values:
        type: map
        required: false
        description: This map will be filled by a modifier and will contain var_name -> var_value.
        entry_schema:
          type: string
      debug_operations:
        type: boolean
        required: false
        default: false
        description: |
          Set to true to make operation more verbose
      container_name:
        type: string
        required: true
        description: |
          the name of the container to use
      batch_size:
        type: integer
        required: false
        default: 5
        description: |
          Number of pods to launch at once in each round of executor pod allocation.
      ca_cert:
        type: string
        required: false
        description: |
          The kube cluster ca cert in base64
      client_cert:
        type: string
        required: false
        description: |
          The kube user cert in base64
      client_key:
        type: string
        required: false
        description: |
          The kube user key in base64
      server:
        type: string
        required: false
        description: |
          The kube api server
      namespace:
        type: string
        required: false
        description: |
          The k8s namespace
      annotations:
        type: map
        required: false
        entry_schema:
          type: string
        description: |
          Annotations to add on driver and executor pods
      labels:
        type: map
        required: false
        entry_schema:
          type: string
        description: |
          Annotations to add on driver and executor pods
      environments:
        type: map
        required: false
        entry_schema:
          type: string
        description: |
          Environment variable to add to driver pod.
      volumes:
        type: list
        required: false
        entry_schema:
          type: org.alien4cloud.k8s.spark.jobs.Volume
        description: |
          Volumes to mount in executor pods
      parameters:
        type: list
        required: false
        entry_schema:
          type: string
      secrets:
        type: map
        required: false
        entry_schema:
          type: string
        description: |
          Secrets to add on driver and executor pods
      executor_limit_cores:
        type: string
        required: false
        description: |
          A hard cpu limit for each executor pod launched for the Spark Application.
      driver_limit_cores:
        type: string
        required: false
        description: |
          A hard cpu limit for the driver pod.
      executor_request_cores:
        type: string
        required: false
        description: |
          The cpu request for each executor pod.
      memory_overhead_factor:
        type: string
        required: false
        description: |
          Memory Overhead Factor that will allocate memory to non-JVM memory.
    attributes:
      kube_config_file_path: { get_operation_output: [SELF, Standard, configure, KUBE_CONFIG_FILE_PATH] }
      config_file_path: { get_operation_output: [SELF, Standard, configure, CONFIG_FILE_PATH] }
      parameter_file: { get_operation_output: [ SELF, Standard, create, PARAMETER_FILE ] }
    capabilities:
      attach: org.alien4cloud.capabilities.DockerVolumeAttachment
    requirements:
      - endpoint:
          capability: org.alien4cloud.capabilities.StaticEndpoint
          relationship: org.alien4cloud.relationships.ConnectsToStaticEndpoint
          occurrences: [0, unbounded]
    interfaces:
      Standard:
        create:
          implementation: scripts/create.sh
        configure:
          inputs:
            var_values: { get_property: [ SELF, var_values ] }
            CA_CERT: { get_property: [SELF, ca_cert] }
            CLIENT_CERT: { get_property: [SELF, client_cert] }
            CLIENT_KEY: { get_property: [SELF, client_key] }
            SERVER: { get_property: [SELF, server] }
            NAMESPACE: { get_property: [SELF, namespace] }
            debug_operations: { get_property: [SELF, debug_operations] }
          implementation: playbook/configure.yml
        delete:
          inputs:
            KUBECONFIG_FILE: { get_attribute: [SELF, kube_config_file_path] }
            PARAMETER_FILE: { get_attribute: [SELF, parameter_file] }
            CONFIG_FILE: { get_attribute: [SELF, config_file_path] }
            debug_operations: { get_property: [SELF, debug_operations] }
          implementation: scripts/delete.sh
      tosca.interfaces.node.lifecycle.Runnable:
        run:
          inputs:
            NAMESPACE: { get_property: [SELF, namespace] }
            KUBECONFIG_FILE: { get_attribute: [SELF, kube_config_file_path] }
            debug_operations: { get_property: [SELF, debug_operations] }
          implementation: scripts/run.sh
        cancel:
          inputs:
            NAMESPACE: { get_property: [SELF, namespace] }
            KUBECONFIG_FILE: { get_attribute: [SELF, kube_config_file_path] }
            debug_operations: { get_property: [SELF, debug_operations] }
          implementation: scripts/cancel.sh
    artifacts:
      - config_folder:
          file: config
          type: tosca.artifacts.File
      - config:
          file: config/config.ja
          type: org.alien4cloud.artifacts.GangjaConfig
      - kube_config:
          file: config/kube_conf.ja
          type: org.alien4cloud.artifacts.GangjaConfig
      - common:
          type: tosca.artifacts.File
          file: scripts/submit-common.sh

  org.alien4cloud.k8s.spark.jobs.JavaSparkJob:
    derived_from: org.alien4cloud.k8s.spark.jobs.AbstractSparkJob
    description: |
      Spark Jobs on K8S.
    metadata:
      icon: /images/spark.png
    properties:
      java_class:
        type: string
        required: true
        description: |
          the java class that implements the job
      jar_file:
        type: string
        required: true
        description: |
          the jar file that implements the job
    interfaces:
      tosca.interfaces.node.lifecycle.Runnable:
        submit:
          inputs:
            CA_CERT: { get_property: [SELF, ca_cert] }
            CLIENT_CERT: { get_property: [SELF, client_cert] }
            CLIENT_KEY: { get_property: [SELF, client_key] }
            SERVER: { get_property: [SELF, server] }
            NAMESPACE: { get_property: [SELF, namespace] }
            CONTAINER_NAME: { get_property: [SELF, container_name] }
            JAR_FILE: { get_attribute: [SELF, jar_file] }
            JAVA_CLASS: { get_property: [SELF, java_class] }
            BATCH_SIZE: { get_property: [SELF, batch_size] }
            ANNOTATIONS: { get_property: [SELF, annotations] }
            LABELS: { get_property: [SELF, labels] }
            ENVIRONMENTS: { get_property: [ SELF, environments] }
            SECRETS: { get_property: [SELF, secrets] }
            VOLUMES: { get_property: [SELF, volumes] }
            PARAMETERS: { get_property: [SELF, parameters] }
            PARAMETER_FILE: { get_attribute: [SELF, parameter_file] }
            EXECUTOR_LIMIT_CORES: { get_property: [SELF, executor_limit_cores] }
            DRIVER_LIMIT_CORES: { get_property: [SELF, driver_limit_cores]}
            EXECUTOR_REQUEST_CORES: { get_property: [SELF, executor_request_cores] }
            MEMORY_OVERHEAD_FACTOR: { get_property: [SELF, memory_overhead_factor] }
            debug_operations: { get_property: [SELF, debug_operations] }
          implementation: scripts/submit-java.sh

  org.alien4cloud.k8s.spark.jobs.PythonSparkJob:
    derived_from: org.alien4cloud.k8s.spark.jobs.AbstractSparkJob
    description: |
      Python Jobs on K8S
    metadata:
      icon: /images/python.png
    properties:
#      py_file:
#        type: string
#        required: true
#        description: |
#          the python file that implements the job
      pythonVersion:
        type: string
        required: false
        description: |
          The python version (spark.kubernetes.pyspark.pythonVersion)
    attributes:
      kube_config_file_path: { get_operation_output: [SELF, Standard, configure, KUBE_CONFIG_FILE_PATH] }
      config_file_path: { get_operation_output: [SELF, Standard, configure, CONFIG_FILE_PATH] }
      python_config_args_file_path: { get_operation_output: [SELF, Standard, configure, PYTHON_CONFIG_ARGS_FILE_PATH] }
    artifacts:    
      - kube_config:
          file: config/kube_conf.ja
          type: org.alien4cloud.artifacts.GangjaConfig
      - config_args:
          file: config/config.ja
          type: org.alien4cloud.artifacts.GangjaConfig
      - config:
          file: config/config.ja
          type: org.alien4cloud.artifacts.GangjaConfig
    interfaces:
      Standard:
        configure:
          inputs:
            var_values: { get_property: [ SELF, var_values ] }
            CA_CERT: { get_property: [SELF, ca_cert] }
            CLIENT_CERT: { get_property: [SELF, client_cert] }
            CLIENT_KEY: { get_property: [SELF, client_key] }
            SERVER: { get_property: [SELF, server] }
            NAMESPACE: { get_property: [SELF, namespace] }
            debug_operations: { get_property: [SELF, debug_operations] }
          implementation: playbook/configure_python.yml
      tosca.interfaces.node.lifecycle.Runnable:
        submit:
          inputs:
            CA_CERT: { get_property: [SELF, ca_cert] }
            CLIENT_CERT: { get_property: [SELF, client_cert] }
            CLIENT_KEY: { get_property: [SELF, client_key] }
            SERVER: { get_property: [SELF, server] }
            NAMESPACE: { get_property: [SELF, namespace] }
            CONTAINER_NAME: { get_property: [SELF, container_name] }
            #PY_FILE: { get_attribute: [SELF, py_file] }
            BATCH_SIZE: { get_property: [SELF, batch_size] }
            ANNOTATIONS: { get_property: [SELF, annotations] }
            LABELS: { get_property: [SELF, labels] }
            ENVIRONMENTS: { get_property: [ SELF, environments]}
            SECRETS: { get_property: [SELF, secrets] }
            VOLUMES: { get_property: [SELF, volumes] }
            PARAMETERS: { get_property: [SELF, parameters]}
            PARAMETER_FILE: { get_attribute: [SELF, parameter_file]}
            EXECUTOR_LIMIT_CORES: { get_property: [SELF, executor_limit_cores]}
            DRIVER_LIMIT_CORES: { get_property: [SELF, driver_limit_cores]}
            EXECUTOR_REQUEST_CORES: { get_property: [SELF, executor_request_cores]}
            MEMORY_OVERHEAD_FACTOR: { get_property: [SELF, memory_overhead_factor]}
            debug_operations: { get_property: [SELF, debug_operations] }
            PYTHON_CONFIG_ARGS_FILE_PATH: { get_attribute: [SELF, python_config_args_file_path] }
            PYTHON_VERSION: { get_property: [SELF, pythonVersion] }
          implementation: scripts/submit-python.sh

  org.alien4cloud.k8s.spark.jobs.AbstractSpark3Job:
    abstract: true
    derived_from: org.alien4cloud.k8s.spark.jobs.AbstractSparkJob
    description: |
      Abstract Spark Jobs on K8S.
    metadata:
      icon: /images/spark.png
    properties:
      context:
        type: string
        required: false
        description: |
         The context from the user Kubernetes configuration file used for the initial auto-configuration of the Kubernetes client library.
      driver_master:
        type: string
        required: false
        description: |
         Internal Kubernetes master (API server) address to be used for driver to request executors.
      annotations_service_driver:
        type: map
        required: false
        entry_schema:
          type: string
        description: |
          Annotations to add on driver service
      driver_request_cores:
        type: string
        required: false
        description: |
          The cpu request for the driver pod
      local_dirs_tmpfs:
        type: string
        required: false
        description: |
           EmptyDir volumes configuration used to back SPARK_LOCAL_DIRS within the Spark driver and executor pods to use tmpfs backing
      kerberos_krb5_path:
        type: string
        required: false
        description: |
           Specify the local location of the krb5.conf file to be mounted on the driver and executors for Kerberos interaction
      kerberos_krb5_config_map_name:
        type: string
        required: false
        description: |
           Specify the name of the ConfigMap, containing the krb5.conf file, to be mounted on the driver and executors for Kerberos interaction
      hadoop_config_map_name:
        type: string
        required: false
        description: |
           Specify the name of the ConfigMap, containing the HADOOP_CONF_DIR files, to be mounted on the driver and executors for custom Hadoop configuration
      kerberos_token_secret_name:
        type: string
        required: false
        description: |
           Specify the name of the secret where your existing delegation tokens are stored. This removes the need for the job user to provide any kerberos credentials for launching a job
      kerberos_token_item_key:
        type: string
        required: false
        description: |
           Specify the item key of the data where your existing delegation tokens are stored. This removes the need for the job user to provide any kerberos credentials for launching a job.
      driver_pod_template_container_name:
        type: string
        required: false
        description: |
           Specify the container name to be used as a basis for the driver in the given pod template.
      executor_pod_template_container_name:
        type: string
        required: false
        description: |
           Specify the container name to be used as a basis for the executor in the given pod template
      executor_delete_on_termination:
        type: boolean
        default: true
        description: |
           Specify whether executor pods should be deleted in case of failure or normal termination.
      submission_connection_timeout:
        type: integer
        default: 10000
        description: |
           Specify whether executor pods should be deleted in case of failure or normal termination.
      submission_request_timeout:
        type: integer
        default: 10000
        description: |
           Request timeout in milliseconds for the kubernetes client to use for starting the driver.
      driver_connection_timeout:
        type: integer
        default: 10000
        description: |
           Connection timeout in milliseconds for the kubernetes client in driver to use when requesting executors.
      driver_request_timeout:
        type: integer
        default: 10000
        description: |
           Request timeout in milliseconds for the kubernetes client in driver to use when requesting executors.
      app_kill_pod_deletion_grace_period:
        type: integer
        required: false
        description: |
           Specify the grace period in seconds when deleting a Spark application using spark-submit.
      file_upload_path:
        type: string
        required: false
        description: |
           Path to store files at the spark submit side in cluster mode.
      driver_memory:
        type: string
        required: false
        description: |
          Amount of memory to be allocated for the driver process.
      executor_memory:
        type: string
        required: false
        description: |
          Amount of memory to be allocated per executor process.
      driver_memory_overhead:
        type: string
        required: false
        description: |
          Amount of additional memory to be allocated for the driver process
      executor_memory_overhead:
        type: string
        required: false
        description: |
          Amount of additional memory to be allocated per executor process.
    attributes:
      driver_pod_template_file_path: { get_operation_output: [SELF, Standard, configure, DRIVER_POD_TEMPLATE_FILE_PATH] }
      executor_pod_template_file_path: { get_operation_output: [SELF, Standard, configure, EXECUTOR_POD_TEMPLATE_FILE_PATH] }
      kube_config_file_path: { get_operation_output: [SELF, Standard, configure, KUBE_CONFIG_FILE_PATH] }
      config_file_path: { get_operation_output: [SELF, Standard, configure, CONFIG_FILE_PATH] }
      parameter_file: { get_operation_output: [ SELF, Standard, create, PARAMETER_FILE ] }
    interfaces:
      Standard:
        create:
          implementation: scripts/create.sh
        configure:
          inputs:
            var_values: { get_property: [ SELF, var_values ] }
            CA_CERT: { get_property: [SELF, ca_cert] }
            CLIENT_CERT: { get_property: [SELF, client_cert] }
            CLIENT_KEY: { get_property: [SELF, client_key] }
            SERVER: { get_property: [SELF, server] }
            NAMESPACE: { get_property: [SELF, namespace] }
            debug_operations: { get_property: [SELF, debug_operations] }
          implementation: playbook/configure-spark3.yml
        delete:
          inputs:
            KUBECONFIG_FILE: { get_attribute: [SELF, kube_config_file_path] }
            DRIVER_POD_TEMPLATE_FILE: { get_attribute: [SELF, driver_pod_template_file_path] }
            EXECUTOR_POD_TEMPLATE_FILE: { get_attribute: [SELF, executor_pod_template_file_path] }
            PARAMETER_FILE: { get_attribute: [SELF, parameter_file] }
            CONFIG_FILE: { get_attribute: [SELF, config_file_path] }
            debug_operations: { get_property: [SELF, debug_operations] }
          implementation: scripts/delete-spark3.sh
      tosca.interfaces.node.lifecycle.Runnable:
        run:
          inputs:
            NAMESPACE: { get_property: [SELF, namespace] }
            KUBECONFIG_FILE: { get_attribute: [SELF, kube_config_file_path] }
            debug_operations: { get_property: [SELF, debug_operations] }
          implementation: scripts/run.sh
    artifacts:
      - driver_pod_template:
          file: config/driverpodtemplate.ja
          type: org.alien4cloud.artifacts.GangjaConfig
          description: Specify the local file that contains the driver pod template
      - executor_pod_template:
          file: config/executorpodtemplate.ja
          type: org.alien4cloud.artifacts.GangjaConfig
          description: Specify the local file that contains the executor pod template


  org.alien4cloud.k8s.spark.jobs.JavaSpark3Job:
    derived_from: org.alien4cloud.k8s.spark.jobs.AbstractSpark3Job
    description: |
      Spark3 Jobs on K8S.
    metadata:
      icon: /images/spark.png
    properties:
      java_class:
        type: string
        required: true
        description: |
          the java class that implements the job
      jar_file:
        type: string
        required: true
        description: |
          the jar file that implements the job
    interfaces:
      tosca.interfaces.node.lifecycle.Runnable:
        submit:
          inputs:
            CA_CERT: { get_property: [SELF, ca_cert] }
            CLIENT_CERT: { get_property: [SELF, client_cert] }
            CLIENT_KEY: { get_property: [SELF, client_key] }
            SERVER: { get_property: [SELF, server] }
            NAMESPACE: { get_property: [SELF, namespace] }
            CONTAINER_NAME: { get_property: [SELF, container_name] }
            JAR_FILE: { get_attribute: [SELF, jar_file] }
            JAVA_CLASS: { get_property: [SELF, java_class] }
            BATCH_SIZE: { get_property: [SELF, batch_size] }
            ANNOTATIONS: { get_property: [SELF, annotations] }
            LABELS: { get_property: [SELF, labels] }
            ENVIRONMENTS: { get_property: [ SELF, environments] }
            SECRETS: { get_property: [SELF, secrets] }
            VOLUMES: { get_property: [SELF, volumes] }
            PARAMETERS: { get_property: [SELF, parameters] }
            PARAMETER_FILE: { get_attribute: [SELF, parameter_file] }
            EXECUTOR_LIMIT_CORES: { get_property: [SELF, executor_limit_cores] }
            DRIVER_LIMIT_CORES: { get_property: [SELF, driver_limit_cores]}
            DRIVER_MEMORY: { get_property: [ SELF, driver_memory ]}
            EXECUTOR_MEMORY: { get_property: [ SELF, executor_memory]}
            DRIVER_MEMORY_OVERHEAD: { get_property: [ SELF, driver_memory_overhead ]}
            EXECUTOR_MEMORY_OVERHEAD: { get_property: [ SELF, executor_memory_overhead]}
            EXECUTOR_REQUEST_CORES: { get_property: [SELF, executor_request_cores] }
            MEMORY_OVERHEAD_FACTOR: { get_property: [SELF, memory_overhead_factor] }
            CONTEXT: { get_property: [SELF, context] }
            DRIVER_MASTER: { get_property: [SELF, driver_master] }
            ANNOTATIONS_SERVICE_DRIVER: { get_property: [SELF, annotations_service_driver] }
            DRIVER_REQUEST_CORES: { get_property: [SELF, driver_request_cores] }
            LOCAL_DIRS_TMPFS: { get_property: [SELF, local_dirs_tmpfs] }
            KERBEROS_KRB5_PATH: { get_property: [SELF, kerberos_krb5_path] }
            KERBEROS_KRB5_CONFIG_MAP_NAME: { get_property: [SELF, kerberos_krb5_config_map_name] }
            HADOOP_CONFIG_MAP_NAME: { get_property: [SELF, hadoop_config_map_name] }
            KERBEROS_TOKEN_SECRET_NAME: { get_property: [SELF, kerberos_token_secret_name] }
            KERBEROS_TOKEN_ITEM_KEY: { get_property: [SELF, kerberos_token_item_key] }
            DRIVER_POD_TEMPLATE_CONTAINER_NAME: { get_property: [SELF, driver_pod_template_container_name] }
            EXECUTOR_POD_TEMPLATE_CONTAINER_NAME: { get_property: [SELF, executor_pod_template_container_name] }
            EXECUTOR_DELETE_ON_TERMINATION: { get_property: [SELF, executor_delete_on_termination] }
            SUBMISSION_CONNECTION_TIMEOUT: { get_property: [SELF, submission_connection_timeout] }
            SUBMISSION_REQUEST_TIMEOUT: { get_property: [SELF, submission_request_timeout] }
            DRIVER_CONNECTION_TIMEOUT: { get_property: [SELF, driver_connection_timeout] }
            DRIVER_REQUEST_TIMEOUT: { get_property: [SELF, driver_request_timeout] }
            APP_KILL_POD_DELETION_GRACE_PERIOD: { get_property: [SELF, app_kill_pod_deletion_grace_period] }
            FILE_UPLOAD_PATH: { get_property: [SELF, file_upload_path] }
            POD_DRIVER_TEMPLATE: { get_attribute: [SELF, driver_pod_template_file_path] }
            POD_EXECUTOR_TEMPLATE: { get_attribute: [SELF, executor_pod_template_file_path] }
            debug_operations: { get_property: [SELF, debug_operations] }
          implementation: scripts/submit-java.sh

  org.alien4cloud.k8s.spark.jobs.PythonSpark3Job:
    derived_from: org.alien4cloud.k8s.spark.jobs.AbstractSpark3Job
    description: |
      Python Jobs on K8S
    metadata:
      icon: /images/python.png
    properties:
#      py_file:
#        type: string
#        required: true
#        description: |
#          the python file that implements the job
      pythonVersion:
        type: string
        required: false
        description: |
          The python version (spark.kubernetes.pyspark.pythonVersion)
    attributes:
      kube_config_file_path: { get_operation_output: [SELF, Standard, configure, KUBE_CONFIG_FILE_PATH] }
      config_file_path: { get_operation_output: [SELF, Standard, configure, CONFIG_FILE_PATH] }
      python_config_args_file_path: { get_operation_output: [SELF, Standard, configure, PYTHON_CONFIG_ARGS_FILE_PATH] }
      driver_pod_template_file_path: { get_operation_output: [SELF, Standard, configure, DRIVER_POD_TEMPLATE_FILE_PATH] }
      executor_pod_template_file_path: { get_operation_output: [SELF, Standard, configure, EXECUTOR_POD_TEMPLATE_FILE_PATH] }
    artifacts:
      - kube_config:
          file: config/kube_conf.ja
          type: org.alien4cloud.artifacts.GangjaConfig
      - config_args:
          file: config/config.ja
          type: org.alien4cloud.artifacts.GangjaConfig
      - config:
          file: config/config.ja
          type: org.alien4cloud.artifacts.GangjaConfig

    interfaces:
      Standard:
        configure:
          inputs:
            var_values: { get_property: [ SELF, var_values ] }
            CA_CERT: { get_property: [SELF, ca_cert] }
            CLIENT_CERT: { get_property: [SELF, client_cert] }
            CLIENT_KEY: { get_property: [SELF, client_key] }
            SERVER: { get_property: [SELF, server] }
            NAMESPACE: { get_property: [SELF, namespace] }
            debug_operations: { get_property: [SELF, debug_operations] }
          implementation: playbook/configure_python-spark3.yml
      tosca.interfaces.node.lifecycle.Runnable:
        submit:
          inputs:
            CA_CERT: { get_property: [SELF, ca_cert] }
            CLIENT_CERT: { get_property: [SELF, client_cert] }
            CLIENT_KEY: { get_property: [SELF, client_key] }
            SERVER: { get_property: [SELF, server] }
            NAMESPACE: { get_property: [SELF, namespace] }
            CONTAINER_NAME: { get_property: [SELF, container_name] }
            #PY_FILE: { get_attribute: [SELF, py_file] }
            BATCH_SIZE: { get_property: [SELF, batch_size] }
            ANNOTATIONS: { get_property: [SELF, annotations] }
            LABELS: { get_property: [SELF, labels] }
            ENVIRONMENTS: { get_property: [ SELF, environments]}
            SECRETS: { get_property: [SELF, secrets] }
            VOLUMES: { get_property: [SELF, volumes] }
            PARAMETERS: { get_property: [SELF, parameters]}
            PARAMETER_FILE: { get_attribute: [SELF, parameter_file]}
            EXECUTOR_LIMIT_CORES: { get_property: [SELF, executor_limit_cores]}
            DRIVER_LIMIT_CORES: { get_property: [SELF, driver_limit_cores]}
            DRIVER_MEMORY: { get_property: [ SELF, driver_memory ]}
            EXECUTOR_MEMORY: { get_property: [ SELF, executor_memory]}
            DRIVER_MEMORY_OVERHEAD: { get_property: [ SELF, driver_memory_overhead ]}
            EXECUTOR_MEMORY_OVERHEAD: { get_property: [ SELF, executor_memory_overhead]}
            EXECUTOR_REQUEST_CORES: { get_property: [SELF, executor_request_cores]}
            MEMORY_OVERHEAD_FACTOR: { get_property: [SELF, memory_overhead_factor]}
            debug_operations: { get_property: [SELF, debug_operations] }
            PYTHON_CONFIG_ARGS_FILE_PATH: { get_attribute: [SELF, python_config_args_file_path] }
            PYTHON_VERSION: { get_property: [SELF, pythonVersion] }
            CONTEXT: { get_property: [SELF, context] }
            DRIVER_MASTER: { get_property: [SELF, driver_master] }
            ANNOTATIONS_SERVICE_DRIVER: { get_property: [SELF, annotations_service_driver] }
            DRIVER_REQUEST_CORES: { get_property: [SELF, driver_request_cores] }
            LOCAL_DIRS_TMPFS: { get_property: [SELF, local_dirs_tmpfs] }
            KERBEROS_KRB5_PATH: { get_property: [SELF, kerberos_krb5_path] }
            KERBEROS_KRB5_CONFIG_MAP_NAME: { get_property: [SELF, kerberos_krb5_config_map_name] }
            HADOOP_CONFIG_MAP_NAME: { get_property: [SELF, hadoop_config_map_name] }
            KERBEROS_TOKEN_SECRET_NAME: { get_property: [SELF, kerberos_token_secret_name] }
            KERBEROS_TOKEN_ITEM_KEY: { get_property: [SELF, kerberos_token_item_key] }
            DRIVER_POD_TEMPLATE_CONTAINER_NAME: { get_property: [SELF, driver_pod_template_container_name] }
            EXECUTOR_POD_TEMPLATE_CONTAINER_NAME: { get_property: [SELF, executor_pod_template_container_name] }
            EXECUTOR_DELETE_ON_TERMINATION: { get_property: [SELF, executor_delete_on_termination] }
            SUBMISSION_CONNECTION_TIMEOUT: { get_property: [SELF, submission_connection_timeout] }
            SUBMISSION_REQUEST_TIMEOUT: { get_property: [SELF, submission_request_timeout] }
            DRIVER_CONNECTION_TIMEOUT: { get_property: [SELF, driver_connection_timeout] }
            DRIVER_REQUEST_TIMEOUT: { get_property: [SELF, driver_request_timeout] }
            APP_KILL_POD_DELETION_GRACE_PERIOD: { get_property: [SELF, app_kill_pod_deletion_grace_period] }
            FILE_UPLOAD_PATH: { get_property: [SELF, file_upload_path] }
            POD_DRIVER_TEMPLATE: { get_attribute: [SELF, driver_pod_template_file_path] }
            POD_EXECUTOR_TEMPLATE: { get_attribute: [SELF, executor_pod_template_file_path] }
          implementation: scripts/submit-python.sh
